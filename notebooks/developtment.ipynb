{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Aggregate a Historical Dataset into Hourly Intervals\n",
    "\n",
    "As a data science expert, you are to construct a new dataset aggregated by the hour from a historical dataset that records data every 5 minutes. The primary focus is on bike sharing stations, using the 'station_id' for alignment and calculating the 'percentage_docks_available' at these stations. Follow these detailed instructions to complete the task:\n",
    "\n",
    "Merge Datasets Instructions:\n",
    "\n",
    "1 - Combine the relevant datasets using the 'station_id' as the key.\n",
    "Remove any records (registries) where 'station_id' does not have a match in both datasets. This ensures only data with complete 'station_id' references are considered.\n",
    "\n",
    "2 - Aggregate Records:\n",
    "Group the data by 'station_id' and the 'last_reported' timestamp. Include index, station_id, month, day, hour columns.\n",
    "\n",
    "3 - Calculate Dock Availability:\n",
    "Within the aggregation, compute the 'percentage_docks_available' for each group. This is done by dividing 'num_docks_available' by 'capacity' for each record and expressing the result as a percentage:\n",
    "\n",
    "percentage_docks_available =(num_docks_available/capacity)\n",
    "\n",
    "Please provide the code based on this datasets:\n",
    "\n",
    "Dataset 1: station_id, num_bikes_available, num_bikes_available_types.mechanical, num_bikes_available_types.ebike, num_docks_available, is_installed, is_renting, is_returning, last_reported, is_charging_station, status, last_updated, ttl\n",
    "\n",
    "Dataset 2: station_id, name, physical_configuration, lat, lon, altitude, address, post_code, capacity, is_charging_station, nearby_distance, _ride_code_support, rental_uris, cross_street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_percentage(group):\n",
    "    mean_docks = group['num_docks_available'].mean()\n",
    "    mean_capacity = group['capacity'].mean()\n",
    "    return (mean_docks / mean_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import dask.dataframe as dd\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src')\n",
    "from data.load_raw_quarter import load_raw_quarter\n",
    "from data.exploration import detect_anomalies\n",
    "\n",
    "# Cargar la información sobre las estaciones\n",
    "stations_df = pd.read_csv(\"../data/raw/Informacio_Estacions_Bicing.csv\")\n",
    "# Convertir station_id a set para búsquedas más rápidas\n",
    "valid_station_ids = set(stations_df['station_id'])\n",
    "stations_df.set_index('station_id', inplace=True)\n",
    "\n",
    "# Cargar los archivos de datos brutos\n",
    "files_df = load_raw_quarter()\n",
    "\n",
    "# New DataFrame para registrar anomalías\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "for file_name, df in tqdm(files_df.items(), desc=\"Processing Files\"):\n",
    "    try:\n",
    "        print(f'START {file_name}'.center(100, \"=\"))\n",
    "\n",
    "        if isinstance(df, dd.DataFrame):\n",
    "            df = df.compute()\n",
    "        \n",
    "        ## Build DataFrame\n",
    "        # Convert 'last_reported' to datetime\n",
    "        new_df['last_reported'] = dd.to_datetime(new_df['last_reported'], unit='s')\n",
    "        # Extract components needed for grouping\n",
    "        new_df['year'] = new_df['last_reported'].dt.year\n",
    "        new_df['month'] = new_df['last_reported'].dt.month\n",
    "        new_df['day'] = new_df['last_reported'].dt.day\n",
    "        new_df['hour'] = new_df['last_reported'].dt.hour\n",
    "        \n",
    "        grouped = new_df.groupby(['station_id', 'year', 'month', 'day', 'hour'])\n",
    "\n",
    "        percentage_docks_available = grouped.apply(calculate_percentage, meta=('x', 'f8')).reset_index()\n",
    "        percentage_docks_available.columns = ['station_id', 'year', 'month', 'day', 'hour', 'percentage_docks_available']\n",
    "        \n",
    "        result = percentage_docks_available.compute()\n",
    "        result.to_csv('hourly_aggregated_data.csv', index=False)\n",
    "\n",
    "        print(f'END {file_name}'.center(100, \"=\"))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
